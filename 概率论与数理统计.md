## 事件的概率

概率：又称或然率、几率，是表示某种情况(事件)出现的可能性大小的一种数量指标，它介于$0$与$1$之间.

事件：概率论中，事件指对某种(或某些)情况的陈述，它可能发生，也可能不发生，发生与否，要到有关的“试验”有了结果以后才能知晓.

“事件”的一般含义：

(1)有一个明确界定的试验

(2)这个试验的全部结果，是在试验前就明确的.

(3)我们有一个明确的陈述，这个陈述界定了试验的全部可能结果中一个确定的部分.这个陈述，或者说一个确定的部分，就叫做一个确定的部分.

基本事件：在概率论上，有时把单一的试验结果称为一个“基本事件”.这样，一个或一些基本事件并在一起，就构成一个事件，而基本事件本身也是事件.

随机事件:若某个事件在某次实验中是否发生取决于机遇，则称其为“随机事件”.随机事件的极端情况是“必然事件”和“不可能事件”.

必然事件:在试验中必然发生的事件称为“必然事件”.

不可能事件:在试验中不可能发生的事件称为“不可能事件”.

可以把必然事件和不可能事件分别等同于概率为$1$和概率为$0$的事件.从严格的理论角度而言，这二者有所区别，但这种区别并无实际的重要性.

概率的性质

等可能概型/古典概型:

假定某个试验有有限个可能的结果$e_1,e_2,\cdots,e_N$.假定从该试验的条件及实施方法上去分析，我们找不到任何理由认为其中某一结果，例如$e_i$,比任一其他结果，例如$e_j(i\ne j)$,更具有优势(即更容易发生)
，于是我们只好认为，所有结果$e_1,e_2\cdots,e_N$在试验中有同等可能的出现机会，即$\frac{1}{N}$的出现机会.我们常常把这样的试验结果称为“等可能的”.

古典概率：

设一个试验有$N$个等可能的结果,而事件$E$恰包含其中的$M$个结果，则事件$E$的概率，记为$P(E)$，定义为：

$$
P(E)=\frac{M}{N}
$$

例(cxr例1.1)

例(cxr例1.2,几何概率)

频率：

反复做大量试验，

概率的统计定义：

把事件$E$的概率定义为具有如下性质的一个数$p$:当把试验重复，$E$的频率在$p$的附近摆动，且当重复次数增大时，这个摆动愈来愈小.或者干脆说，概率就是当试验次数无限增大时频率的极限

概率的公理化定义：

排列：

$$
P^n_r
=n(n-1)(n-2)\cdots(n-r+1)
=\frac{n!}{(n-r)!}
$$

组合：

$$
\tbinom{n}{r}
=\frac{P^n_r}{r!}
=\frac{n(n-1)(n-2)\cdots(n-r+1)}{r!}
=\frac{n!}{r!(n-r)!}
$$

古典概率计算：

例(cxr例2.1)：

例(cxr例2.2)：

$n$双相异的鞋公$2n$只，随机地分成$n$堆，每堆$2$只.问“各堆都自成一双鞋”这个事件$E$的概率是多少？

答案：$P(E)=\frac{1}{(2n-1)!!},$其中，$"!!"$这个符号对**奇自然数**地定义为：$a!!=1\cdot3\cdot 5\cdots a$

解法一：

$2n$双相异的鞋，随机分成$n$堆，每堆$2$只，计较排列顺序的话，共有排法：

$$
N
=\tbinom{2n}{2}\tbinom{2n-2}{2}\cdots\tbinom{2}{2}
=\frac{(2n)!}{2^n}
$$

$2n$双相异的鞋，随机分成$n$堆，每堆$2$只，且恰好各堆都自成一双鞋，计较排列顺序的话，共有排法：

$$
M=n!
$$

于是：

$$
P(E)=\frac{M}{N}=\frac{n!2^n}{(2n)!}=\frac{1}{(2n-1)!!}
$$

解法二：

同样有：

$$
N
=\tbinom{2n}{2}\tbinom{2n-2}{2}\cdots\tbinom{2}{2}
=\frac{(2n)!}{2^n}
$$

不同的是$M$的计算：

$$
M=(2n)\cdot(2n-2)\cdots(2)=n!2^n
$$

于是：

$$
P(E)=\frac{M}{N}=\frac{1}{(2n-1)!!}
$$


例(cxr例2.3):

$n$个男孩，$m$个女孩$(m\leqslant n+1)$随机地排成一列.问“任意两个女孩都不相邻”这个事件$E$的概率是多少？

答案：$P(E)=\frac{\tbinom{n+1}{m}}{\tbinom{n+m}{m}}$

解法(插空法)：

所有人随机地排成一列，总共有排法：

$$
N=(n+m)!
$$

所有人随机地排成一列，且任意两个女孩不相邻的排法：

$$
M=n!\tbinom{n+1}{m}m!
$$

于是：

$$
P(E)
=\frac{M}{N}
=\frac{n!\tbinom{n+1}{m}m!}{(n+m)!}
=\frac{\tbinom{n+1}{m}}{\tbinom{n+m}{m}}
$$

例(cxr例2.4)：

答案：$P=\frac{\tbinom{2n-m}{n}}{2^{2n-m}}$

解法一：

例(cxr例2.5):

答案：$P=\frac{17!21!}{17^{21}2!^3 3!^4 4!5!6!} $

例(cxr例2.6):

答案：$P=1-\frac{1}{2^{n-1}} $

### 事件的运算、条件概率与独立性

在实用上和理论上，下述情况常见：问题中有许多比较简单的事件，其概率易于算出，或是有了理论上的假定值，或是根据以往的经验已对其值作了充分精确的估计.而我们感兴趣的是一个复杂的事件$E,$它通过种种关系与上述简单
事件联系起来，这时我们想设法利用这种联系，以便从这些简单事件的概率去算出$E$的概率.正如在微积分中，直接利用定义可算出若干简单函数的导数，但利用导数所满足的法则，可据此算出很复杂的函数的导数.

事件的蕴含、包含及相等：

在**同一试验下**的两个事件$A$和$B$，如果当$A$发生时$B$必发生，则称$A$蕴含$B$，或者说$B$包含$A$，记为$A\subset B$.若$A,B$相互蕴含，即:$A\subset B$且$B\subset A$，则称$A,B$两事件相等，记为：$A=B$

拿“事件是试验的一些结果”这个观点来看，若$A$蕴含$B$，那只能是：$A$中的试验结果必在$B$中，即$B$这个集合(作为试验结果的集合)要大一些，“包含”一词由此而来.实际含义是：若$A \subset B$(也写为$B \supset A$)，则$A$和$B$相比，更难发生一些，因而其概率就必然小于或至多等于$B$的概率.“两事件$A,B$”相等，无非是说，$A,B$由完全同一的一些试验结果构成，它不过是同一事件表面上看来不同的两个说法而已.

证明两事件相等的一般方法是：先设事件$A$发生，由此推出事件$B$发生；再反过来，由假定$B$发生推出$A$发生.

事件的互斥和对立：

互斥事件

若两事件$A,B$不能在同一次试验中都发生(但可以都不发生)，则称它们是互斥的.如果一些事件中任意两个都互斥，则称这些事件是两两互斥的，或简称互斥的.

从“事件是一些试验结果所构成的”这个观点看，互斥事件无非是说，构成这两个事件各自的试验结果中不能有公共的.

对立事件：

互斥事件的一个重要情况是“对立事件”.若$A$为一事件，则事件：

$$
B=\{A不发生 \}
$$

称为$A$的对立事件，多记为$\bar{A} $(读作$A~~ bar$，也记为$A^c$)

对立事件也常称为“补事件”.若两个事件互为对立事件，则它们的和是必然事件，它们的交集是空集.

事件的和(或称并)：

设有两个事件$A$和$B$，**定义**一个新事件$C$如下：

$$
C=\{A发生，或B发生 \}=\{A,B至少发生一个 \}
$$

所谓**定义一个事件**，就是指出它何时发生，何时不发生.在上面的定义下，$C$在何时发生呢？只要$A$发生或者$B$发生(或二者同时发生也可以)，就算是$C$发生了，不然(即$A,B$都不发生)则算作$C$不发生.这样定义的事件$C$称为事件$A$与事件$B$的和，记为：

$$
C=A+B
$$

有时也记为$C=A\bigcup B$，不过本书采用$C=A+B$

两事件的和，即把构成各自事件的那些试验结果并在一起所构成的事件

这样，若$C=A+B$，则$A,B$都蕴含$C$，$C$包含$A$也包含$B$.加过相加，事件变得更大了(含有了更多的试验结果)，因而更容易发生了.

事件的和很自然地推广到多个事件的情形，

概率的加法定律：

定理：

若干互斥事件之和的概率，等于各事件的概率之和，即：若$A_1,A_2,\cdots,$两两互斥，则有：

$$
P(A_1+A_2+\cdots)=P(A_1)+P(A_2)+\cdots
$$

上述定理中，事件个数可以是有限的，也可以是无限的.结论成立的重要条件(或者说前提)是**各事件必须两两互斥**.

证明(概率的古典定义下)：

加法定理的一个重要推论：

以$\bar{A} $表示$A$的对立事件，则：

$$
P(\bar{A})=1-P(A)
$$

事件的积(或称交)、事件的差

设有两个事件$A,B$，则如下定义的事件$C$:

$$
C={A,B都发生}
$$

称为两事件$A,B$的积或乘积，并记为$AB$.一般地，事件$A,B$各是一些试验结果的集合，而$AB$则由同属于这两个集合的哪些试验结果组成，即这两个集合的交叉.按积的定义，两个事件$A,B$互斥，等于说$AB$是不可能事件.

多个事件的积的定义

两个事件$A,B$之差，记为$A-B$，定义为：

$$
A-B=\{A发生,B不发生 \}
$$

一般地，$A-B$就是从构成$A$的那些试验中，去掉在$B$内的那一些

$$
A-B=A\bar{B}
$$

运算规律：

上面引进了和、积、差等运算，下面是成立的运算规则：

$$
A+B=B+A \\
AB=BA \\
A(B-C)=AB-AC
$$

第三行证明如下：

条件概率：

一般来讲，条件概率就是附加在一定条件之下所计算的概率.从广义的意义上说，任何概率都是条件概率，因为我们是在一定的试验之下去考虑事件的概率的，而试验即规定有条件.在概率论中，决定试验的那些基础条件被看作已定不变的.如果不再加入其他条件或假定，则算出的概率就叫做“无条件概率”，就是通常所说的概率.当说到“条件概率”时，总是指另外附加的条件，其形式总可归结为“已知某事件发生了”.

在古典概率的模式下分析一般的情况：

设一试验有$N$个等可能的结果，事件$A,B$分别包含其中的$M_1$个和$M_2$个结果，它们有$M_{12}$是公共的，这就是事件$AB$所包含的试验结果数.若已给定$B$发生，则我们的考虑由起先的$N$个可能结果局限到现在的$M_2$个，其中只有$M_{12}$个试验结果使事件$A$发生，故一个合理的条件概率定义，应把$P(A|B)$取为$\frac{M_{12}}{M_2}$.但：

$$
\frac{M_{12}}{M_2}
=\frac{M_{12}/N}{M_2 /N}=\frac{P(AB)}{P(B)}
$$

由此得出如下的一般定义：

设有两个事件$A,B$，而$P(B)\ne 0$.则“在给定$B$发生的条件下$A$的条件概率”，记为$P(A|B)$，定义为：

$$
P(A|B)=\frac{P(AB)}{P(B)}
$$

上面是条件概率的一般定义，但在计算条件概率时，并不一定要用它.有时，直接从加入条件后改变了的情况去算，更为方便.

例(cxr例3.1):

答案：$\frac{5}{18} $

事件的独立性，概率乘法定理：

若$P(A)=P(A|B),$则$B$的发生与否对$A$发生的可能性毫无影响.这时，在概率论上就称$A,B$两事件独立.

定义：称两个事件$A,B$独立，若它们满足以下条件：

$$
P(A)P(B)=P(AB)
$$

定理：两独立事件$A,B$的积$AB$的概率$P(AB)$等于其各自概率的积$P(A)P(B)$

在实际问题中，我们并不常用上式去判断两个事件是否独立，而是相反：从事件的实际角度去分析判断其不应有关联，因而是独立的，然后就可以用上式

多个事件独立性的定义：

概率乘法定理：

若干个独立事件$A_1,\cdots,A_n$之积的概率，等于各事件概率的乘积：

$$
P(A_1\cdots A_n)=P(A_1)\cdots P(A_n)
$$

推论：

独立事件的任一部分也独立

若一系列事件$A_1,A_2,\cdots$相互独立，则将其中任一部分改为对立事件时，所得事件列仍相互独立.

相互独立和两两独立：

由互相独立必推出两两独立；由两两独立不一定推出互相独立

例(cxr例3.4)：

$E=\{飞机被击落 \}，$ $E_0=\{驾驶员被击中 \}，E_i=\{i号发动机被击中 \}(i=1,2),$规定：$E=E_0+E_1E_2,$设$E_0,E_1,E_2$三事件独立,$P(E_0)=p_0,P(E_1)=p_1,P(E_2)=p_2$，求$P(E)$

思路：首先，要区分“独立”与“互斥”的概念.“独立”则可以用概率乘法定理，“互斥”则可以用概率加法定理.这里$E_0,E_1,E_2$三事件独立，但不互斥，因为$E_0,E_1,E_2$可以同时发生.

法一：

$$
P(E)
=P(E_0+E_1E_2)
=P(E_0)+P(E_1E_2)-P(E_0E_1E_2)
=p_0+p_1p_2-p_0p_1p_2
$$

法二(考虑逆事件)：

考虑：

$$
P(\overline{E})
=P(\overline{E_0+E_1E_2})
=P(\mathop{\overline{E_0}}\mathop{\overline{E_1E_2}})
=P(\overline{E_0})P(\overline{E_1E_2})
=(1-P(E_0))(1-P(E_1E_2))
=(1-P(E_0))(1-P(E_1)P(E_2))
=(1-p_0)(1-p_1p_2)
$$

于是：

$$
P(E)
=1-P(\overline{E})
=p_0+p_1p_2-p_0p_1p_2
$$

例(cxr例3.5,很难)：



由于$E_0,E_1,E_2$三事件独立，所以他们的逆事件也独立，于是他们的逆事件满足概率乘法定理，


全概率公式：

设$B_1,B_2,\cdots$为有限或无限个事件，他们两两互斥且在每次试验中至少发生一个.数学表达式为：

$$
B_iB_j=\varnothing,~~~(i\neq j) \\
B_1+B_2+\cdots =\Omega~~~(必然事件)
$$

有时，把具有这些性质的一组事件称为一个“**完备事件群**”.显然，任一事件$B$及其对立事件组成一个完备事件群

在上述条件下考虑任一事件$A,$有：

$$
A
=A\Omega
=A(B_1+B_2+\cdots)
=AB_1+AB_2+\cdots
$$

因为$B_1,B_2,\cdots$两两互斥，所以$AB_1,AB_2,\cdots$也两两互斥，故由加法定理，有：

$$
P(A)
=P(AB_1+AB_2+\cdots)
=P(AB_1)+P(AB_2)+\cdots
$$

由条件概率定义：$P(AB_i)=P(B_i)P(A|B_i), $有：

$$
P(A)
=P(B_1)P(A|B_1)+P(B_2)P(A|B_2)+\cdots
$$

上式就称为“**全概率公式**”

“全”部概率被分成了许多部分之和.它的理论和实用意义在于：在较复杂的情况下直接算$P(A)$不容易，但$A$总是随某个$B_i$伴出，适当构造这一组$B_i$往往可以简化运算

另一个角度理解全概率公式：把$B_i$看作导致事件$A$发生的一种可能途径
.对不同途径，$A$发生的概率，即条件概率$P(A|B_i)$各不相同，而采取哪条途径却是随机的.在直观上容易理解：在这种机制下，$A$的综合概率$P(A)$应在最小的$P(A|B_i)$和最大的$P(A|B_i)$之间,但$P(A)$也不一定是所有$P(A|B_i)$的算术平均值，因为各途径被使用的机会$P(B_i)$各不相同.$P(A)$正确的答案,应该是诸$P(A|B_i)(i=1,2,\cdots)$以$P(B_i)$为权的加权平均值

例(cxr例3.7)：

设一个家庭有$k$个小孩的概率为$p_k(k=0,1,2,\cdots).$又设各小孩的性别独立，且生男、生女的概率各为$\frac{1}{2}.$试求事件$A=\{家庭中所有小孩为统一性别\}$的概率(约定当$k=0$时,$P(A|k=0)=1$).

引进事件$B_k=\{家庭中有k各小孩 \}，$则$B_0,B_1,\cdots$构成完备事件群.由题知，$P(B_k)=p_k$.由全概率公式：$P(A)=P(B_0)P(A|B_0)+P(B_1)P(A|B_1)+\cdots$，有:

当$k\geqslant 1,$

$$
P(A|B_k)
=2\cdot\frac{1}{2^k}
=\frac{1}{2^{k-1}}
$$

于是：

$$
P(A)
=p_0+\sum_{k=1}^\infty \frac{p_k}{2^{k-1}}
$$

### 贝叶斯公式

在全概率公式假定之下，有：

$$
P(B_i|A)
=\frac{P(B_i)P(A|B_i)}{\sum_{j}P(B_j)P(A|B_j)}
$$

证明：

$$
P(B_i|A)
=\frac{P(AB_i)}{P(A)}
=\frac{P(B_i)P(A|B_i)}{\sum_{j}P(B_j)P(A|B_j)}
$$

对贝叶斯公式的认识：

先看$P(B_1),P(B_2),\cdots,$它们是在没有进一步的信息(不知事件$A$是否发生)的情况下，人们对诸事件$B_1,B_2,\cdots$发生可能性大小的认识；现在有了新的信息(知道$A$发生)，人们对$B_1,B_2,\cdots$发生可能性大小有了新的估价.这种情况在日常生活中也是屡见不鲜的：原以为不甚可能的一种情况，可以因某种事件的发生而变得甚为可能；或者相反.贝叶斯公式从数量上刻画了这种变化.

如果我们把事件$A$看成“结果”，把诸事件$B_1,B_2,\cdots$看成导致这个结果的可能的“原因”
，则可以形象地把**全概率公式**看作“**由原因推结果**”；而**贝叶斯公式**则恰好相反，其作用在于“**由结果推原因**”：现在有一个“结果”$A$已经发生了，在众多可能的“原因”中，到底是哪一个导致了这个结果？

例(cxr例3.9):

设某种病菌在人口中的带菌率为$0.03$,当检查时，由于各种原因，使带菌者未必检出阳性而不带菌者也可能呈阳性反应，假定：

$$
P(阳性|带菌)=0.99,P(阴性|带菌)=0.01 \\
P(阳性|不带菌)=0.05,P(阴性|不带菌)=0.95
$$

现设某人检出阳性，问“他带菌”的概率是多少？

解：

记$B_1=\{带菌\},B_2=\{不带菌\},A=\{阳性\},\bar{A}=\{阴性\}$

则已知条件可改写为：

$$
P(B_1)=0.03,P(B_2)=0.97 \\
P(A|B_1)=0.99,P(\bar{A}|B_1)=0.01 \\
P(A|B_2)=0.05,P(\bar{A}|B_2)=0.95
$$

由贝叶斯公式，得：

$$
P(B_1|A)
=\frac{P(B_1)P(A|B_1)}{P(B_1)P(A|B_1)+P(B_2)P(A|B_2)}
=\frac{0.03\times 0.99}{0.03\times 0.99+0.97\times0.05}
=0.380
$$



随机变量：

设随机试验的样本空间为$S=\{e\}.X=X(e)$是定义在样本空间$S$上的实值单值函数.称$X=X(e)$为随机变量.

离散型随机变量：

称一个随机变量是离散型随机变量，若它全部可能取到的值是有限个或可列无限多个.(q:啥叫可列无限多个？)

$(0-1)$分布：

设随机变量$X$只可能取$0,1$两个值，它的分布律是：

$$
P\{X=k \}=p^k(1-p)^{1-k},k=0,1(0<p<1)
$$

则称$X$服从以$p$为参数的$(0-1) $分布或两点分布

伯努利试验:

设试验$E$只有两个可能结果：$A$和$\bar{A}$,则称$E$为伯努利试验.设：$P(A)=p(0<p<1) ,$ 则：$P(\bar{A})=1-p $

$n$重伯努利试验:

将$E$(只有两种可能地试验)独立重复地进行$n$,则称这一串重复地独立试验为$n$**重伯努利试验**

二项分布：

以$X$表示$n$重伯努利试验中事件$A$发生的次数，$p$为事件$A$发生的概率，$X$是一个随机变量，我们称随机变量$X$服从参数为$n,p$的二项分布，记为：

$$
X\sim b(n,p)
$$

$X$的分布律为：

$$
P\{X=k\}
=\tbinom{n}{k}p^k(1-p)^{n-k} ，k=0,1,2,\cdots ,n
$$

泊松分布:

设随机变量$X$所有可能取得值为$0,1,2,\cdots$,而取各个值得概率为：
$$
P\{X=k\}=\frac{\lambda^ke^{-\lambda}}{k!},k=0,1,2,\cdots
$$
其中，$\lambda>0$是常数.则称$X$服从参数为$\lambda$的泊松分布.

泊松分布的性质(想到$e^x$的泰勒展开)：

$$
\sum_{k=0}^\infty P\{X=k\}
=\sum_{k=0}^\infty \frac{\lambda^ke^{-\lambda}}{k!}
=e^{-\lambda}\sum_{k=0}^\infty\frac{\lambda^k}{k!}
=e^{-\lambda}e^\lambda
=1
$$

泊松定理(将二项分布近似为泊松分布)：

设$\lambda>0 $是一个常数，n是任意正整数，设$np_n=\lambda$,则对于任一固定的非负整数$k$,有：
$$
\lim_{n\to\infty}\tbinom{n}{k}p_n^k(1-p_n)^{n-k}
=\frac{\lambda^ke^{-\lambda}}{k!}
$$

证明：

当$n$很大，$p$很小，为参数的二项分布的概率值可以由参数为$\lambda=np$的泊松分布的概率值近似.

随机变量的分布函数：

设$X$是一个随机变量，$x$是任意实数，函数：

$$
F(x)=P\{X\leqslant x\},-\infty<x<\infty
$$
称为$X$的分布函数.

对于任意实数$x_1,x_2(x_1 < x_2) $ ,有：
$$
P\{x_1<X\leqslant x_2\}=p\{x\leqslant x_2\}-P\{x\leqslant x_1\}=F(x_2)-F(x_1)
$$

连续型随机变量：

若对于随机变量$X$的分布函数$F(x)$,存在非负可积函数$f(x) $,使对于任意实数有：

$$
F(x)=\int_{-\infty}^xf(t)dt
$$

则称$X$为连续型随机变量，$f(x)$称为$X$的概率密度函数，简称概率密度.

概率密度的性质：

1.$f(x)\geqslant0$ \
2.$\int_{-\infty}^\infty f(x)dx=1 $\
3.对于任意实数$x_1,x_2(x_1\leqslant x_2)$,
$$
P\{x_1<X\leqslant x_2\}=F(x_2)-F(x_1)=\int_{x_1}^{x_2}f(x)dx
$$

4.若$f(x) $在$x$点连续，则有：$F'(x)=f(x)$

若$f(x)$具有性质1,2,则可引入：

$$
G(x)=\int_{-\infty}^x f(t)dt
$$
$G(x) $是某一随机变量$X$的函数，$f(x) $是$X$的概率密度.


均匀分布：

若连续型随机变量$X$具有概率密度：

$$
f(x)=
\begin{cases}
\frac{1}{b-a}&,&a<x<b \\
0&, &其他
\end{cases}
$$

则称$X$在区间$(a,b) $上服从均匀分布，记为:
$$
X\sim U(a,b)
$$

$X$的分布函数为：

$$
F(x)=
\begin{cases}
0&,x<a \\
\frac{x-a}{b-a}&,a\leqslant x<b \\
1 &,x\geqslant b
\end{cases}
$$

指数分布：

若连续型随机变量$X$的概率密度为：

$$
f(x)=
\begin{cases}
\frac{1}{\theta}e^{-\frac{x}{\theta}}&,x>0 \\
0&,其他
\end{cases}
$$

其中$\theta>0$为常数，则称$X$服从参数为$\theta$的指数分布.

$X$的分布函数为：
$$
F(x)=
\begin{cases}
1-e^{-\frac{x}{\theta}}&,x>0 \\
0&,其他
\end{cases}
$$
性质：

无记忆性：

若随机变量$X$服从指数分布，则对于任意$s,t>0$,有:

$$
P\{X>s+t|X>s\}=P\{X>t\}
$$
证明：

$$
\begin{aligned}
P\{X>s+t |X>s \}
&=\frac{P\{(X>s+t)\bigcap(X>s) \}}{P\{X>s \}} \\
&=\frac{P{\{X>s+t \}}}{P\{X>s \}} \\
&=\frac{1-F(s+t)}{1-(s)} \\
&=\frac{e^{-\frac{s+t}{\theta}}}{e^{-\frac{s}{\theta}}} \\
&=e^{-\frac{t}{\theta}}
\end{aligned}
$$

而：
$$
P\{X>t \}=e^{-\frac{t}{\theta}}
$$

故：

$$
P\{X>s+t|X>s\}=P\{X>t\}
$$

正态分布：

称连续型随机变量$X$服从参数为$\mu,\sigma$的正态分布或高斯分布，记为$X\sim N(\mu,\sigma^2)$,若连续型随机变量的概率密度为：
$$
f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},-\infty<x<\infty
$$
其中，$\mu,\sigma(\sigma>0)$为常数.

证明此概率密度满足归一化：


首先有结论：
$$
\int_{-\infty}^\infty e^{-\frac{t^2}{2}}\mathrm{d}t=\sqrt{2\pi} 
$$

性质：

曲线关于$x=\mu$对称.这表明对任意$h>0 $有：

$$
P\{\mu-h<X\leqslant\mu\}
=
P\{\mu<X\leqslant \mu+h \}
$$

当$x=\mu $时取到最大值.

在$x=\mu \pm \sigma $处曲线有拐点,曲线以$Ox$轴为渐进线.

当$\mu=0,\sigma=1$时，称随机变量$X$服从**标准正态分布**，其概率密度和分布函数分别用$\varphi(x),\Phi(x) $表示，即有：

$$
\varphi(x)
=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \\
\Phi(x)
=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-\frac{t^2}{2}}\mathrm{d}t 

$$
性质：

$$
\Phi(x)+\Phi(-x) = 1
$$

证明：

若随机变量$X \sim N(\mu,\sigma^2) $,则$Z=\frac{X-\mu}{\sigma} \sim N(0,1) $

证明：

$$
P\{Z\leqslant x\}
=P\{\frac{X-\mu}{\sigma}\leqslant x \}
=P\{X\leqslant \sigma x+\mu\}
=\int_{-\infty}^{\sigma x+\mu} \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(t-\mu)^2}{2\sigma^2}}\mathrm{d}t
$$

应用：

若随机变量$X\sim N(\mu,\sigma^2)$,$X$的分布函数为$F$，则：

$$
F(x)
=P\{X\leqslant x \}
=P\{\frac{X-\mu}{\sigma}\leqslant \frac{x-\mu}{\sigma} \}
=P\{Z\leqslant \frac{x-\mu}{\sigma} \}
=\Phi(\frac{x-\mu}{\sigma})
$$

对于任意区间$(x_1,x_2]$,有：

$$
P\{x_1<X \leqslant x_2 \}
=\Phi(\frac{x_2-\mu}{\sigma})-\Phi(\frac{x_1-\mu}{\sigma})
$$

例：





### 随机变量函数的分布：

离散型随机变量：

连续型随机变量：

定理：

SOP:

开始，先由$X,Y$关系确定$Y$的取值范围.如：$Y=X^2,有Y\geqslant 0$则当$y\leqslant 0,F_Y(y)=P\{Y\leqslant y \}=0$或者直接$f_Y(y)=0$?；如$Y=\sqrt{X},有Y\geqslant 0 $


接着求$Y$的分布函数$F_Y(y)=P\{Y\leqslant y\}$，由$X,Y$关系把$Y$用$X$表达，$P\{Y\leqslant y\}=P\{h(X)\leqslant y \}=P\{X\leqslant h^{-1}(y)\}=F_X(h^{-1}(y))$于是，$F_Y(y)=F_X(h^{-1}(y))$，最后两边对$y$求导：$f_Y(y)=f_X(h^{-1}(y))\cdot\frac{\mathrm{d}}{\mathrm{d}y}h^{-1}(y) $


例（已知一个随机变量$X$的概率密度、另一随机变量$Y$与$X$的关系式，求$Y$的概率密度）：

设随机变量$X$具有概率密度：

$$
f_X(x)=
\begin{cases}
\frac{x}{8}&,0<x<4 \\
0&,其他
\end{cases}
$$

求随机变量$Y=2X+8$的概率密度

记$X,Y$的分布函数分别为：$F_X(x),F_Y(y)$

$$
F_Y(y)
=P\{Y\leqslant y\}
=P\{2X+8\leqslant y \}
=P\{X\leqslant \frac{y-8}{2} \}
=F_X(\frac{y-8}{2})
$$

两边同时对$y$求导：

$$
f_Y(y)
=f_X(\frac{y-8}{2})(\frac{y-8}{2})'
=
\begin{cases}
\frac{1}{2}\frac{\frac{y-8}{2}}{8} &, 0<\frac{y-8}{2} <4 \\
0 &,其他
\end{cases}
=
\begin{cases}
\frac{y-8}{32} &, 8<y<16 \\
0 &, 其他
\end{cases}
$$

例：

设随机变量$X$具有概率密度$f_X(x),-\infty<x<\infty $,求 $Y=X^2$ 的概率密度

当$Y\leqslant 0,F_Y(y)=0,$或者直接$f_Y(y)=0$更直接？

当$Y\geqslant 0,$

$$
F_Y(y)
=P\{Y\leqslant y\}
=P\{X^2\leqslant y\}
=P\{-\sqrt{y}\leqslant X\leqslant\sqrt{y}\}
=F_X(\sqrt{y})-F_X(-\sqrt{y})
$$

两边对$y$求导：

当$y>0,$

$$
f_Y(y)
=\frac{1}{2\sqrt{y}}\bigg(f_X(\sqrt{y})+f_X(\sqrt{-y})\bigg)
$$

综上，

$$

f_Y(y)=

\begin{cases}

\frac{1}{2\sqrt{y}}\bigg(f_X(\sqrt{y})+f_X(\sqrt{-y})\bigg) &,y>0 \\
0 &,y\leqslant 0 

\end{cases}

$$

例：

设随机变量$X\sim N(\mu,\sigma^2)$,证明$Z=\frac{x-\mu}{\sigma}\sim N(0,1) $

例：




## 多维随机变量及其分布

设$E$是一个随机试验，它的样本空间是$S=\{e\}$,设$X=X(e)$和$Y=Y(e)$是定义在$S$上的随机变量，由它们构成的一个向量$(X,Y) $叫做**二维随机向量**或**二维随机变量**

二维随机变量的**分布函数**/两个随机变量的**联合分布函数** ：

设$(X,Y) $是二维随机变量，对于任意实数$x,y$,二元函数：

$$
F(x,y)=P\{(X\leqslant x)\bigcap (Y\leqslant y) \}\stackrel{记成}{=}P\{X\leqslant x,Y\leqslant y \}
$$

称为二维随机变量$(X,Y) $的**分布函数**，或称为随机变量$X$和$Y$的联合分布函数.

几何意义：

若把二维随机变量$(X,Y) $看作平面上随机点的坐标，那么分布函数$F(x,y) $在$(x,y) $处的函数值就是随机点$(X,Y) $落在以点$(x,y)$为顶点而位于该点左下方的无穷矩形域内的概率.

借助几何图像，易知：

随机点$(X,Y)$落在矩形域$\{(x,y)|x_1<x\leqslant x_2,y_1<y\leqslant y_2 \} $的概率为：

$$
\begin{aligned}
&P\{x_1<X\leqslant x_2,y_1<Y\leqslant y_2 \ \} \\
=&F(x_2,y_2)-F(x_1,y_2)-F(x_2,y_1)+F(x_1,y_1)
\end{aligned}
$$

分布函数$F(x,y) $的基本性质：

$F(x,y) $是变量$x,y$的不减函数

$0\leqslant F(x,y)\leqslant 1 $

对于任意固定的$y,F(-\infty,y)=0 $

对于任意固定的$x,F(x,-\infty)=0 $

$F(-\infty,-\infty)=0,F(\infty,\infty)=1$

$F(x,y) $关于$x$右连续，关于$y$也右连续

对于任意$(x_1,y_1),(x_2,y_2),x_1<x_2,y_1<y_2 $,下述不等式成立：

$$
F(x_2,y_2)+F(x_1,y_1)-F(x_1,y_2)-F(x_2,y_1)\geqslant 0
$$

若二维随机变量$(X,Y)$全部可能取到的值是有限对或可列无限对，则称$(X,Y) $是二维离散型随机变量.

设二维随机变量$(X,Y)$所有可能取值为：$(x_i,y_i) ,i,j=1,2,\cdots$
记$P=\{X=x_i,Y=y_i \}=p_{ij},i,j=1,2,\cdots,$则由概率定义：

$$
p_{ij}\geqslant 0,\sum_{i=0}^\infty\sum_{j=0}^\infty p_{ij} =1
$$

我们称$P=\{X=x_i,Y=y_i \}=p_{ij},i,j=1,2,\cdots$为二维离散型随机变量$(X,Y) $的分布律，或称为随机变量$X$和$Y$的联合分布律.

求离散型二维随机变量$(X,Y)$分布律SOP：

1.写出$X,Y$各自可能取值，并制成表

2.计算特定概率

例：

离散型随机变量$X$和$Y$的联合分布函数：

$$
F(x,y)
=\sum_{x_i\leqslant x}\sum_{y_j\leqslant y}p_{ij}
$$

其中，和式是对一切满足$x_i\leqslant x,y_j\leqslant y $的$i,j$来求和的.

对于二维随机变量$(X,Y) $分分布函数$F(x,y) ,$若存在非负可积函数$f(x,y)$使对于任意$x,y$有：

$$
\int_{-\infty}^y\int_{-\infty}^x f(u,v)\mathrm{d}u\mathrm{d}v
$$

则称$(X,Y) $是二维连续型随机变量,函数$f(x,y) $称为二维连续型随机变量$(X,Y)$的概率密度，或称为随机变量$X$和$Y$的联合概率密度.

概率密度$f(x,y) $的性质：

$$f(x,y)\geqslant 0$$

$$
\int_{-\infty}^\infty\int_{-\infty}^\infty f(u,v)\mathrm{d}u\mathrm{d}v=1
$$

(important!)设$G$是$xOy$平面上的区域，点$(X,Y) $落在$G$内的概率为：

$$
P\{(X,Y)\in G \}
=\underset{G}\iint f(x,y)\mathrm{d}x\mathrm{d}y
$$

若$f(x,y) $在点$(x,y) $处连续，则有：

$$
\frac{\partial^2F(x,y) }{\partial x\partial y}=f(x,y)
$$

由性质4：

在$f(x,y) $的连续点处有：
$$
\lim_{\Delta x\to 0^+\Delta y\to 0^+} \frac{P\{x<X\leqslant x+\Delta x,y<Y\leqslant y+\Delta y \}}{\Delta x\Delta y}
=\frac{\partial^2F(x,y) }{\partial x\partial y}=f(x,y)
$$

这表示，若$f(x,y)$在点$(x,y) $处连续，则当$\Delta x,\Delta y $很小时，有：

$$
P\{x<X\leqslant x+\Delta x,y<Y\leqslant y+\Delta y \}
\approx
f(x,y)\Delta x\Delta y
$$

也就是点$(X,Y) $落在小矩形$(x,x+\Delta x]\times(y,y+\Delta y] $内的概率接近$f(x,y)\Delta x\Delta y $

在几何上，$z=f(x,y) $表示空间的一个曲面.由性质2知，介于它和$xOy$平面的空间区域的体积为$1$.由性质3，$P\{(X,Y)\in G \} $的值等于以$G$为底，以曲面$z=f(x,y) $为顶面的柱体体积.


一般，设$E$是一个随机试验，它的样本空间是$S=\{e\}$,设$X_1=X_1(e),X_2=X_2(e),\cdots,X_n=X_n(e) $是定义在$S$上的随机变量，由它们构成一个$n$维向量$(X_1,X_2,\cdots,X_n)$称为$n$维随机向量或$n$维随机变量.

对于任意$n$个实数$x_1,x_2,\cdots,x_n ,n$元函数：

$$
F(x_1,x_2,\cdots,x_n)=P\{X_1\leqslant x_1,X_2\leqslant x_2,\cdots,X_n\leqslant x_n \}
$$

称为$n$维随机变量$(X_1,X_2,\cdots,X_n)$的分布函数或随机变量$X_1,X_2,\cdots,X_n$的联合分布函数.

## 边缘分布

边缘分布函数：

边缘概率密度：

二维正态分布：

$$
(X,Y)\sim N(\mu_1,\mu_2,\sigma_1,\sigma_2.\rho)
$$

二维正态分布的两个边缘分布都是一维正态分布

### 条件分布：

若$P\{Y=y_j\}>0$，则在$Y=y_j$条件下随机变量$X$的条件分布律为：

$$
P\{X=x_i|Y=y_j\}
=\frac{P\{X=x_i,Y=y_j \}}{P\{Y=y_j\}}
=\frac{p_{ij}}{p_{\cdot j}},
i=1,2,\cdots.
$$

若$P\{X=x_i \}>0$，则在$X=x_i$条件下随机变量$Y$的条件分布律为：

$$
P\{Y=y_j|X=x_i\}=\frac{P\{X=x_i,Y=y_j \}}{P\{X=x_i\}}=\frac{p_{ij}}{p_{i\cdot}} ,j=1,2,\cdots
$$



条件概率的性质：

1.

2.

$$
\sum_{i=1}^{\infty}P\{X=x_i|Y=y_j \}=1
$$

条件分布律：

条件概率密度：

若对于固定的$Y,f_Y(y)>0,$则称$\frac{f(x,y)}{f_Y(y)} $为在$Y=y$条件下$X$的条件概率密度，记为：

$$
f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)}
$$

称$\int_{-\infty}^x f_{X|Y}(x|y)\mathrm{d}x=\int_{-\infty}^x \frac{f(x,y)}{f_Y(y)}\mathrm{d}x $为在$Y=y$条件下$X$的**条件分布函数**，记为:$P\{X\leqslant x|Y=y\} $或$F_{X|Y}(x,y) $,即：

$$
F_{X|Y}(x,y)=P\{X\leqslant x|Y=y \}=\int_{-\infty}^x \frac{f(x,y)}{f_Y(y)}\mathrm{d}x
$$

类似地，可以定义：

条件分布函数：

均匀分布：



### 相互独立的随机变量



### 两个随机变量的函数分布

#### $Z=X+Y$ 的分布

设$(X,Y)$是二维连续型随机变量，它具有概率密度$f(x,y) $，则$Z=X+Y$仍为随机变量，其概率密度为：

$$
f_{X+Y}(z)=\int_{-\infty}^\infty f(x,z-x)\mathrm{d}x 
$$

$$
或
$$

$$
f_{X+Y}(z)=\int_{-\infty}^\infty f(z-y,y)\mathrm{d}y
$$

证明：

$$
F_{X+Y}(z)
=P\{X+Y\leqslant z\}
=\underset{x+y\leqslant z}{\iint} f(x,y)\mathrm{d}x\mathrm{d}y
=
$$

卷积公式：

设随机变量$X,Y$相互独立，二维随机变量$(X,Y)$的边缘概率密度分别为 $f_X(x),f_Y(y)$,则：

$$
f_{X+Y}(z)=\int_{-\infty}^\infty f_X(x)f_Y(z-x)\mathrm{d}x
$$

$$
或
$$

$$
f_{X+Y}(z)=\int_{-\infty}^\infty f_X(z-y)f_Y(y)\mathrm{d}y
$$

卷积公式记为：

$$
f_X*f_Y
$$

#### $Z=\frac{Y}{X}$的分布、$Z=XY$的分布

设$(X,Y)$是二维连续型随机变量，它具有概率密度$f(x,y),$则$Z=\frac{Y}{X},Z=XY $仍为连续型随机变量，其概率密度分别为：

$$
f_{\frac{Y}{X}}(z)
=\int_{-\infty}^\infty |x|f(x,xz)\mathrm{d}x
$$

$$
f_{XY}(z)
=\int_{-\infty}^\infty \frac{1}{|x|}f(x,\frac{z}{x})\mathrm{d}x
$$

证明：

设$(X,Y)$是二维连续型随机变量，若$X,Y$相互独立，设$(X,Y)$关于$X,Y$的边缘概率密度分别为：$f_X(x),f_Y(y) ,$则：

$$
f_{\frac{Y}{X}}(z)
=\int_{-\infty}^\infty |x|f_X(x)f_Y(xz)\mathrm{d}x
$$

$$
f_{XY}(z)
=\int_{-\infty}^\infty \frac{1}{|x|}f_X(x)f_Y(\frac{z}{x})\mathrm{d}x
$$

例：

设随机变量$(X,Y)$的概率密度为：

$$

f(x,y)=

\begin{cases}

x+y &,0<x<1,0<y<1 \\
0 &,其他
\end{cases}

$$

求$Z=XY$的概率密度

$$
f_{XY}
=\int_{-\infty}^\infty \frac{1}{|x|}f(x,\frac{z}{x})\mathrm{d}x
=

\begin{cases}

2-2z  &,0<z<1\\
0 &,其他

\end{cases}

$$



#### $M=\max\{ X,Y\}$及$N=\min\{X,Y \} $的分布

设$X,Y$是两个相互独立的随机变量，它们的分布函数分别为$F_X(x),F_Y(y) $,则：

$$
F_{\max}(z)
=F_X(z)F_Z(z)
$$

$$
F_{\min}(z)
=1-[1-F_X(z)][1-F_Y(z)]
$$

推广：



## 随机变量的数字特征

### 数学期望

设离散型随机变量$X$的分布律为：

$$

$$

$$
E(X)
=a_1p_1+a_2p_2+\cdots+a_mp_m
$$

设$X$有概率密度$f(x),$若：

则$X$的数学期望，记为$E(X)$,可定义为：

$$
E(X)
=\int_{-\infty}^\infty xf(x)\mathrm{d}x
$$

数学期望的性质：

若干个随机变量之和的期望等于各随机变量的期望之和. 数学表达式：

$$
E(X_1+X_2+\cdots+X_n)=E(X_1)+E(X_2)+\cdots+E(X_n)
$$

若干个**独立**随机变量之积的期望等于各变量的期望之积. 数学表达式：

若$X_1,X_2,\cdots,X_n$相互独立，则：

$$
E(X_1X_2\cdots X_n)
=E(X_1)E(X_2)\cdots E(X_n)
$$

### 随机变量函数的期望

设随机变量$X$为离散型，有分布$P(X=a_i)=p_i(i=1,2,\cdots);$或者$X$为连续型，有概率密度$f(x),$则：

$$
E(g(X))
=\sum_{i} g(a_i)p_i
$$

或

$$
E(g(X))
=\int_{-\infty}^\infty g(x) f(x)\mathrm{d}x
$$

推论：

若$a,b$为常数，则：

$$
E(aX+b)=aE(X)+b
$$

### 方差

设$X$为随机变量，其分布为$F,$，其方差，记为$Var(X),$的定义为：

$$
Var(X)
=E\bigg(\big(X-E(X)\big)^2 \bigg)
$$

标准差：$\sqrt{Var(X)}$

方差计算：

$$
Var(X)
=E(X^2)-E^2(X)
$$

方差的性质：

常数的方差为零

若$a,b$为常数，则：

$$
Var(aX+b)
=a^2Var(X)
$$

定理：

**独立**随机变量之和的方差等于各变量的方差之和. 数学表达式：

若$X_1,X_2,\cdots,X_n$相互独立，则：

$$
Var(X_1+X_2+\cdots+X_n)
=Var(X_1)+Var(X_2)+\cdots+Var(X_n)
$$

### 矩

设$X$为随机变量，$c$为常数，$k$为正整数. $X$关于$c$点的$k$阶矩，定义为：

$$
E\bigg((X-c)^k\bigg)
$$

两种特殊情况：





## 协方差与相关系数

协方差：

$$
Cov(X,Y)
=E\bigg(\big(X-E(X)\big)\big(Y-E(Y)\big)\bigg)
$$

协方差的性质：

$$
Cov(c_1X+c_2,c_3Y+c_4)=c_1c_3Cov(X,Y)
$$

$$
Cov(X,Y)=E(XY)-E(X)E(Y)
$$

定理：

若$X,Y$ 相互独立，则：$Cov(X,Y)=0$

$[Cov(X,Y)]^2\leqslant \sigma_1^2\sigma_2^2 .$等号成立当且仅当$X,Y$之间存在严格线性关系(即存在常数$a,b$使得$Y=aX+b$)



相关系数：

$\sigma_1=\sqrt{Var(X)},\sigma_2=\sqrt{Var(Y)}$

$$
Corr(X,Y)
=\frac{Cov(X,Y)}{\sigma_1\sigma_2}
$$

定理：

若$X,Y$独立，则：$Corr(X,Y)=0$

$|Corr(X,Y)|\leqslant 1, $等号取到当且仅当$X,Y$有严格线性关系

称$X,Y$不相关，若$Corr(X,Y)=0$或$Cov(X,Y)=0$

定理一说，$X,Y$独立能推出他们不相关，但反过来一般不成立

## 大数定理和中心极限定理

**独立同分布**：称随机变量$X_1,\cdots,X_n,\cdots$独立同分布，若$X_1,\cdots,X_n,\cdots$互相独立，且服从统一分布

$\bar{X}_n$的定义：

定义$X_i$为：

$$
X_i=

\begin{cases}

1&,若在第i次试验时事件A发生 \\
0&,若在第i次试验时事件A不发生

\end{cases}

$$

在上面对$X_i$的定义之下，定义频率$\bar{X}_n$:

$$
\bar{X}_n=\frac{X_1+\cdots+X_n}{n}
$$

### 马尔可夫不等式

若$Y$为只取非负值的随机变量，则对任意常数$\varepsilon>0,$有：

$$
P(Y\geqslant \varepsilon)\leqslant \frac{E(Y)}{\varepsilon}
$$

证明：

设$Y$的概率密度函数为$f(y),$由期望的定义，结合$Y\geqslant 0,$有：

$$
E(Y)
=\int_{-\infty}^{+\infty}yf(y)\mathrm{d}y
=\int_0^{+\infty}yf(y)\mathrm{d}y
$$

另一方面，

$$
P(Y\geqslant \varepsilon)
=\int_\varepsilon^{+\infty}f(y)\mathrm{d}y
\leqslant \int_\varepsilon^{+\infty}\frac{y}{\varepsilon} f(y)\mathrm{d}y
=\frac{1}{\varepsilon}\int_{\varepsilon}^{+\infty}yf(y)\mathrm{d}y
\leqslant \frac{1}{\varepsilon}\int_{0}^{+\infty}yf(y)\mathrm{d}y
=\frac{E(Y)}{\varepsilon}
$$

也就是说：

$$
P(Y\geqslant \varepsilon)\leqslant \frac{E(Y)}{\varepsilon}
$$

### 切比雪夫不等式

切比雪夫不等式是马尔可夫不等式的一个重要特例

若$Var(Y)$存在，则：

$$
P(|Y-E(Y)|\geqslant \varepsilon)\leqslant \frac{Var(Y)}{\varepsilon^2}
$$

证明：

由马尔可夫不等式：$P(X\geqslant \varepsilon)\leqslant \frac{E(X)}{\varepsilon} ,$把$X=(Y-E(Y))^2,\varepsilon^2$代替$E,\varepsilon$得到：

$$
P(|Y-E(Y)|\geqslant \varepsilon)\leqslant \frac{Var(Y)}{\varepsilon^2}
$$

### 大数定理

设$X_1,\cdots,X_n,\cdots$是独立同分布的随机变量，记它们共同的数学期望为$a,$又设它们的方差存在并记为$\sigma^2.$则对任意给定的$\varepsilon>0,$有：

$$
\lim_{n\to\infty}P(|\bar{X}_n-a|\geqslant \varepsilon)=0
$$

其中，定义$X_i$为：

$$
X_i=

\begin{cases}

1&,若在第i次试验时事件A发生 \\
0&,若在第i次试验时事件A不发生

\end{cases}

$$

在上面对$X_i$的定义之下，定义频率$\bar{X}_n$为:

$$
\bar{X}_n=\frac{X_1+\cdots+X_n}{n}
$$

证明：

对任意$\varepsilon>0,$

由切比雪夫不等式，有：

$$
P(|\bar{X}_n-E(\bar{X}_n)|\geqslant \varepsilon)
\leqslant \frac{Var(\bar{X}_n)}{\varepsilon^2} \tag{1}
$$

注意到：

$$
E(\bar{X}_n)
=E(\frac{X_1+\cdots+X_n}{n})
=\frac{1}{n}E(X_1+\cdots+X_n)
=\frac{1}{n}\bigg(E(X_1)+\cdots+E(X_n)\bigg)
=\frac{1}{n}\cdot\underbrace{(a+\cdots+a)}_{n个a}
=a
$$

又注意到，$\bar{X}_n=\frac{X_1+\cdots+X_n}{n},$而$X_1,\cdots,X_n$相互独立，于是：

$$
Var(\bar{X}_n)
=\frac{1}{n^2}(Var(x_1)+\cdots+Var(X_n))
=\frac{1}{n^2}(\sigma^2+\cdots+\sigma^2)
=\frac{\sigma^2}{n}
$$

把上面两式代入(1)，得到：

$$
P(|\bar{X}_n-a|\geqslant \varepsilon)
\leqslant \frac{\sigma^2}{n\varepsilon^2}
$$

取极限，得：

$$
\lim_{n\to\infty}P(|\bar{X}_n-a|\geqslant \varepsilon)
\leqslant \lim_{n\to\infty}\frac{\sigma^2}{n\varepsilon^2}
=0
$$

又由概率性质，概率恒大于等于零，于是由夹逼定理：

$$
\lim_{n\to\infty}P(|\bar{X}_n-a|\geqslant\varepsilon)=0
$$

### 伯努利大数定理

伯努利大数定理是大数定理的一个特例，可以描述为“频率收敛于概率”

$\bar{X}_n$是频率，记为$p_n;$概率记为$p,$对任意$\varepsilon>0,$有：

$$
\lim_{n\to\infty}P(|p_n-p|\geqslant \varepsilon)=0
$$


### 中心极限定理

设$X_1,\cdots,X_n,\cdots$为独立同分布的随机变量，$E(X_i)=a,Var(X_i)=\sigma^2(0<\sigma<\infty),$则对任何实数$x$，有：

$$
\lim_{n\to\infty}P(\frac{1}{\sqrt{n}\sigma}(X_1+\cdots+X_n-na)\leqslant x)=\varPhi(x)
$$

其中，$\varPhi(x)$是标准正态分布$N(0,1)$的分布函数，即：

$$
\varPhi(x)
=\frac{1}{\sqrt{2\pi}}\int_{{-\infty}}^{x}e^{-\frac{t^2}{2}}\mathrm{d}t
$$

### 拉普拉斯定理

设$X_1,\cdots,X_n,\cdots$独立同分布，$X_i$的分布是：

$$
P(X_i=1)=p,P(X_i=0)=1-p~~~(0<p<1)
$$

则对任何实数$x$，有：

$$
\lim_{n\to\infty}P\bigg(\frac{1}{\sqrt{np(1-p)}}(X_1+\cdots+X_n-np)\leqslant x\bigg)
=\varPhi(x)
$$

## 第三章习题



(cxr)8(貌似比想象中难得多啊，暂时放弃):设$n$为自然数，$f(x)=\frac{c}{(1+x^2)^n} .$找常数$c,$使$f(x)$为概率密度函数，并计算其均值、方差

思路：利用概率密度归一化求$c$

解：

由归一化：

$$
\int_{-\infty}^{+\infty} f(x)\mathrm{d}x=1
$$

代入得：

$$
\int_{-\infty}^{+\infty}\frac{c}{(1+x^2)^n}\mathrm{d}x=1
$$

解得：

$$

$$

(cxr)













