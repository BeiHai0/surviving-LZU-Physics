

# 统计学习及监督学习概论

统计学习方法可以概括如下：

从给定的、有限的、用于学习的训练数据(training data)集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间(hypothesis)；应用某个评价标准(evaluation)，从假设空间中选取一个最优模型，使它对已知的训练数据及未知的测试数据(test data)在给定的评价准则下有最优的预测；最优模型的选取由算法实现.。这样，统计学习方法包括模型的假设空间、模型选择的准则以及模型学习的算法，统称其为统计学习的三要素，简称为模型(model)、策略(strategy)和算法(algorithm)。

实现统计学习方法的步骤如下：

(1)：得到一个有限的训练数据集合；

(2)：确定包含所有可能的模型的假设空间，即学习模型的集合；

(3)：确定模型选择的准则，即学习的策略；

(4)：实现求解最优模型的算法，即学习的算法；

(5)：通过学习方法选择最优模型；

(6)：利用学习的最优模型对新数据进行预测或分析。


# k近邻法

k近邻法(k-nearest neighbor,k-NN)

$L_p$距离($L_p$ distance):设 $\chi$ 是 $n$ 维实数向量空间 $\mathbb{R}^n,x_i,x_j\in \chi,x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^\mathrm{T},x_j=$,则$x_i,x_j$的$L_p$距离定义为：

$$
L_p(x_i,x_j)
=\bigg(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)} |^p \bigg)^\frac{1}{p}
$$

两种特殊情况：

当$p=1,$

$$
L_1(x_i,x_j)
=\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)} |
$$

$L_1$称为曼哈顿距离(Manhattan distance)

当$p=2,$

$$
L_2(x_i,x_j)
=\bigg(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)} |^2 \bigg)^\frac{1}{2}
$$

$L_2$称为欧氏距离(Euclidean distance)


当$p=\infty,$

$$
L_\infty(x_i,x_j)
=\max_l{|x_i^{(l)}-x_j^{(l)} |}
$$

对$p=\infty$时，$L_p$表达式的证明(太麻烦了，懒得打了)：

由$L_p$距离定义：

$$

\begin{aligned}

L_\infty(x_i,x_j)
&=\lim_{p \to \infty}L_p(x_i,x_j) \\
&=\lim_{p \to \infty}\bigg(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)} |^p \bigg)^\frac{1}{p} \\
&=\lim_{p \to \infty}

\end{aligned}

$$


# 朴素贝叶斯法

## 基本方法

设输入空间 $\mathcal{X}\subseteq \mathbb{R}^n $ 为 $n$ 维向量的集合，输出空间为类标记集合 $\mathcal{Y}=\{c_1,c_2,\cdots,c_K \}. $ 输入为特征向量 $\vec{x}\in\mathcal{X} ,$ 输出为类标记 $y\in\mathcal{Y}. $ $X$ 是定义在输入空间 $\mathcal{X} $ 上的随机向量，$Y$ 是定义在输出空间 $\mathcal{Y}$ 上的随机变量. $P(X,Y)$ 是 $X,Y$ 的联合概率分布.

训练数据集：

$$
T=\{(\vec{x}_1,y_1),(\vec{x}_2,y_2),\cdots,(\vec{x}_N,y_N) \}
$$

由 $P(X,Y)$ 独立同分布产生.

朴素贝叶斯法通过数据集学习联合概率分布 $P(X,Y)$

具体来说，学习以下先验概率分布及条件概率分布：

先验概率分布：

$$
P(Y=c_k),~~~k=1,2,\cdots,K
$$

条件概率分布：

$$
P(X=\vec{x}|Y=c_k)
=P( X^{(1)}=\vec{x}^{(1)},X^{(2)}=\vec{x}^{(2)},\cdots,X^{(n)}=\vec{x}^{(n)} | Y=c_k )
$$

于是学习到联合概率分布 $P(X,Y),$ 即由条件概率公式：$P(X=\vec{x},Y=c_k)=P(Y=c_k)P(X=\vec{x} | Y=c_k)$

朴素贝叶斯法对条件概率分布作了条件独立性的假设，即:

$$

\begin{align}

P(X=\vec{x}|Y=c_k)
&\equiv P(X^{(1)}=\vec{x}^{(1)},\cdots,X^{(n)}=\vec{x}^{(n)}|Y=c_k) \\
&=\prod_{j=1}^n P(X^{(j)}=\vec{x}^{(j)}|Y=c_k) 

\end{align}

$$

朴素贝叶斯法进行分类时，对给定的输入 $\vec{x},$ 通过学习到的模型计算后验概率分布：$P(Y=c_k|X=\vec{x});$而由贝叶斯定理(条件概率公式+全概率公式)，有：

$$
P(Y=c_k|X=\vec{x})
=\frac{P(X=\vec{x},Y=c_k)}{P(X=\vec{x})}
=\frac{P(Y=c_k)P(X=\vec{x}|Y=c_k)}{\sum\limits_{i} P(Y=c_i)P(X=\vec{x}|Y=c_i)}
$$

把(2)代入上式，进一步有：

$$
P(Y=c_k|X=\vec{x})
=\frac{P(Y=c_k)\prod\limits_{j}^n P(X^{(j)}=\vec{x}^{(j)}|Y=c_k) }{\sum\limits_{i}P(Y=c_i)\prod\limits_{j}P(X^{(j)}=\vec{x}^{(j)}|Y=c_i)} ,~~~k=1,2,\cdots,K
$$

于是朴素贝叶斯分类器可表示为：

$$
y
=f(\vec{x})
=\arg\max_{c_k}P(Y=c_k|X=\vec{x})
=\arg\max_{c_k}\frac{P(Y=c_k)\prod\limits_{j} P(X^{(j)}=\vec{x}^{(j)}|Y=c_k) }{\sum\limits_{i}P(Y=c_i)\prod\limits_{j}P(X^{(j)}=\vec{x}^{(j)}|Y=c_i)}
$$

注意到，上式中，对所有的 $c_k,$ 分母的值都是相同的，于是,朴素贝叶斯分类器可进一步简化为：

$$
y
=\arg\max_{c_k}P(Y=c_k)\prod\limits_{j} P(X^{(j)}=\vec{x}^{(j)}|Y=c_k) 
$$

### 后验概率最大化

朴素贝叶斯把实例分到后验概率最大的类中，这等价于期望风险最小化.

把实例分到后验概率最大的类所对应的分类决策函数 $f:$

$$
f(\vec{x})
=\arg\max_{y\in\mathcal{Y}}P(Y=y|X=\vec{x})
$$

下面证明，期望风险最小化等价于后验概率最大化.

假设选择$0-1$ 损失函数：

$$
L(Y,f(X))
=

\begin{cases}

1&,Y\ne f(X) \\
0&,Y=f(X)

\end{cases}

$$

设有一组 $(\vec{x},y) ,$ 其中，$\vec{x} $ 是输入的特征向量，$y$ 是输出. 设 $\mathcal{F}$ 是假设空间，我们希望找到一个函数 $f\in \mathcal{F},$ 使得在此函数的作用下，输出的预测值 $f(\vec{x})$ 等于或尽可能接近输出 $y $. 我们选择用“期望风险最小”来描述这种“尽可能接近”


$$

\begin{aligned}

R_{\exp}(f)
&=E[L(Y,f(X))] \\
&=\sum_{i}\sum_{k} L(c_k,f(\vec{x}_i))\cdot P(Y=c_k,X=\vec{x}_i) \\
&=\sum_{k}\sum_{i } L(c_k,f(\vec{x}_i))\cdot P(Y=c_k,X=\vec{x}_i) \\

\end{aligned}

$$


```
期望风险最小就是说，分类决策函数 $f$ 应当输出使得期望风险函数最小的  $y,$即：
```


$$
f(\vec{x})
=\arg\min_{y\in\mathcal{Y}} \sum_{k=1}^{K} L(c_k,y)P(Y=c_k|X=\vec{x})
$$

```
其中，$f(X)$ 是分类决策函数. 这时，期望风险函数为：

$$

\begin{aligned}

R_{\exp}(f)
&=E[L(Y,f(X))] \\
&=\sum_{i}\sum_{k} L(c_k,\vec{x}_i)\cdot P(Y=c_k,X=\vec{x}_i) \\
&=\sum_{i}\sum_{k} L(c_k,\vec{x}_i)\cdot P(Y=c_k,X=\vec{x}_i) \\

\end{aligned}

$$
```

### 朴素贝叶斯的参数估计

极大似然估计：

在朴素贝叶斯法中，学习意味着估计 $P(Y=c_k)$ 和 $P(X^{(j)}=x^{(j)}|Y=c_k)$

先验概率 $P(Y=c_k)$ 的极大似然估计是：

$$
P(Y=c_k)
=\frac{\sum\limits_{i=1}^N I(y_i=c_k)}{N},~~~k=1,2,\cdots,K
$$

设第 $j$ 个特征 $\vec{x}^{(j)} $ 可能取值的集合为 $\{a_{j1},a_{j2},\cdots,a_{jS_j} \},$ 条件概率 $P(X^{(j)}=a_{jl}|Y=c_k) $ 的极大似然估计是：

$$
P(X^{(j)}=a_{jl}|Y=c_k)
=\frac{P(X^{(j)}=a_{jl},Y=c_k)}{P(Y=c_k)}
=\frac{\sum\limits_{i=1}^N I(\vec{x}_i^{(j)}=a_{jl},y_i=c_k) /N}{\sum\limits_{i=1}^{N} I(y_i=c_k)/N}
=\frac{\sum\limits_{i=1}^{N} I(\vec{x}_i^{(j)}=a_{jl},y_i=c_k) }{\sum\limits_{i=1}^N I(y_i=c_k)}
$$

$$
j=1,2,\cdots,n;~~~l=1,2,\cdots,S_j;~~~k=1,2,\cdots,K
$$

式中，$\vec{x}_i^{(j)} $ 是第 $i$ 个样本的输入 $\vec{x}_i$ 的第 $j$ 个特征；$a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值；$I$ 为指示函数.

朴素贝叶斯算法(naive Vayes algorithm)

输入：训练数据 $T= \{(\vec{x}_1,y_1),(\vec{x}_2,y_2),\cdots,(\vec{x}_N,y_N) \},$ 其中 $\vec{x}_i=(\vec{x}_i^{(1)},\vec{x}_i^{(2)},\cdots,\vec{x}_i^{(N)})^{\mathrm{T}},$ $\vec{x}_i^{(j)}$ 是第 $i$ 个样本的输入 $\vec{x}_i $ 的第 $j$ 个特征. $\vec{x}_i^{(j)} \in \{a_{j1},a_{j2},\cdots,a_{jS_j} \} ,$ $a_{jl}$ 是第 $j$ 个特征可能取得第 $l$ 个值，$j=1,2,\cdots,n;l=1,2,\cdots,S_j;y_i\in \{c_1,c_2,\cdots,c_K \};$ 实例 $\vec{x}$ 

输出：实例 $\vec{x}$ 的分类

(1)计算先验概率及条件概率

$$
P(Y=c_k)
=\frac{\sum\limits_{i=1}^N I(y_i=c_k)}{N},~~~k=1,2,\cdots,K
$$

$$
P(X^{(j)}=a_{jl}|Y=c_k)
=\frac{\sum\limits_{i=1}^N I(\vec{x}_i^{(j)}=a_{jl},y_i=c_k)}{\sum\limits_{i=1}^N I(y_i=c_k)}
$$

$$
j=1,2,\cdots,n;~~~l=1,2,\cdots,S_j;~~~k=1,2,\cdots K
$$

(2)对于给定的实例 $\vec{x}=(\vec{x}^{(1)},\vec{x}^{(2)},\cdots,\vec{x}^{(n)})^{\mathrm{T}}, $ 计算：

$$
P(Y=c_k)\prod_{j=1}^{n} P(X^{(j)}=\vec{x}^{(j)}|Y=c_k),~~~k=1,2,\cdots,K
$$

(3)确定实例 $\vec{x}$ 的类：

$$
y=\argmax_{c_k} P(Y=c_k)\prod_{j=1}^{n} P(X^{(j)}=\vec{x}^{(j)}|Y=c_k)
$$

例：

试从下表给定的训练数据学习一个朴素贝叶斯分类器，并确定 $\vec{x}=(2,S)^{\mathrm{T}} $ 的类标记 $y.$ 表中，$X^{(1)},X^{(2)}$ 为特征，取值的集合分别为 $A_1=\{1,2,3 \} ,A_2=\{S,M,L \},$ $Y$ 为标记类，$Y\in C=\{1,-1 \}$ 

|训练数据编号|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|$X^{(1)}$ |1|1|1|1|1|2|2|2|2|2|3|3|3|3|3|
|$X^{(2)}$ |S|M|M|S|S|S|M|M|L|L|L|M|M|L|L|
|$Y$ |-1|-1|1|1|-1|-1|-1|1|1|1|1|1|1|1|-1|

根据朴素贝叶斯算法:

(1):

$$
P(Y=1)=\frac{9}{15}
$$

$$
P(Y=-1)=\frac{6}{15}
$$

$$
P(X^{(1)}=1 |Y=1)=\frac{2}{9},
P(X^{(1)}=2 |Y=1)=\frac{3}{9},
P(X^{(1)}=3 |Y=1)=\frac{4}{9}
$$

$$
P(X^{(2)}=S |Y=1)=\frac{1}{9},
P(X^{(2)}=M |Y=1)=\frac{4}{9},
P(X^{(2)}=L |Y=1)=\frac{4}{9}
$$



$$
P(X^{(1)}=1 |Y=-1)=\frac{3}{6},
P(X^{(1)}=2 |Y=-1)=\frac{2}{6},
P(X^{(1)}=3 |Y=-1)=\frac{1}{6}
$$

$$
P(X^{(2)}=S |Y=-1)=\frac{3}{6},
P(X^{(2)}=M |Y=-1)=\frac{2}{6},
P(X^{(2)}=L |Y=-1)=\frac{1}{6}
$$

对于给定的实例 $\vec{x}=(2,S)^{\mathrm{T}}, $计算：

$$
P(Y=1)P(X^{(1)}=2|Y=1)P(X^{(2)}=S|Y=1)
=\frac{9}{15}\cdot\frac{3}{9}\cdot\frac{1}{9}
=\frac{1}{45}
$$

$$
P(Y=-1)P(X^{(1)}=2|Y=-1)P(X^{(2)}=S|Y=-1)
=\frac{6}{15}\cdot\frac{2}{6}\cdot\frac{3}{6}
=\frac{1}{15}
$$

由于 $P(Y=-1)P(X^{(1)}=2|Y=-1)P(X^{(2)}=S|Y=-1)$ 最大，所以 $y=-1$

### 贝叶斯估计

用极大似然估计可能会出现所要估计的概率值为 $0$ 的情况，这时会影响到后验概率的计算结果，使分类产生偏差。为了解决这一问题，我们采用贝叶斯估计。

条件概率的贝叶斯估计是：

$$
P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)
=\frac{\lambda+\sum\limits_{i=1}^{N} I(\vec{x}_i^{(j)}=a_{jl},y_i=c_k) }{S_j\lambda+\sum\limits_{i=1}^{N} I(y_i=c_k) }
$$

式中，$\lambda\geqslant 0.$贝叶斯估计就是说，在随机变量各个取值的频数上赋予一个正数 $\lambda>0.$ 当 $\lambda=0$ 时就是极大似然估计. 常取 $\lambda=1,$ 这时称为拉普拉斯平滑(Laplacian smoothing). 对任何 $l=1,2,\cdots,S_j;k=1,2,\cdots,K,$ 有：

$$
P_{\lambda}(X^{(j)}=a_{jl}=Y=c_k)>0
$$

$$
\sum_{l=1}^{S_j} P(X^{(j)}=a_{jl}|Y=c_k) =1
$$

上面两式表明，条件概率的贝叶斯估计确实是一种概率分布.

例：

对同样的表，按照拉普拉斯平滑估计概率，即取 $\lambda=1.$





