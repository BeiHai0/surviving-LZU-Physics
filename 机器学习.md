


# k近邻法

k近邻法(k-nearest neighbor,k-NN)

$L_p$距离($L_p$distance):设 $\chi$ 是 $n$ 维实数向量空间 $\mathbb{R}^n,x_i,x_j\in \chi,x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^\mathrm{T},x_j=$,则$x_i,x_j$的$L_p$距离定义为：

$$
L_p(x_i,x_j)
=\bigg(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)} |^p \bigg)^\frac{1}{p}
$$

两种特殊情况：

当$p=1,$

$$
L_1(x_i,x_j)
=\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)} |
$$

$L_1$称为曼哈顿距离(Manhattan distance)

当$p=2,$

$$
L_2(x_i,x_j)
=\bigg(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)} |^2 \bigg)^\frac{1}{2}
$$

$L_2$称为欧氏距离(Euclidean distance)


当$p=\infty,$

$$
L_\infty(x_i,x_j)
=\max_l{|x_i^{(l)}-x_j^{(l)} |}
$$

对$p=\infty$时，$L_p$表达式的证明(太麻烦了，懒得打了)：

由$L_p$距离定义：

$$

\begin{aligned}

L_\infty(x_i,x_j)
&=\lim_{p \to \infty}L_p(x_i,x_j) \\
&=\lim_{p \to \infty}\bigg(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)} |^p \bigg)^\frac{1}{p} \\
&=\lim_{p \to \infty}

\end{aligned}

$$


# 朴素贝叶斯法

朴素贝叶斯法通过数据集学习联合概率分布

朴素贝叶斯法对条件概率分布作了条件独立性的假设:

$$

\begin{align}

P(X=x|Y=c_k)
&\equiv P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k) \\
&=\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k) 

\end{align}

$$

朴素贝叶斯法进行分类时，对给定的输入 $x,$通过学习到的模型计算后验概率分布：$P(Y=c_k|X=x);$而由贝叶斯定理，有：

$$
P(Y=c_k|X=x)
=\frac{P(Y=c_k)P(X=x|Y=c_k)}{\sum\limits_{i} P(Y=c_i)P(X=x|Y=c_i)}
$$

把(2)代入上式，进一步有：

$$
P(Y=c_k|X=x)
=\frac{P(Y=c_k)\prod\limits_{j}^n P(X^{(j)}=x^{(j)}|Y=c_k) }{\sum\limits_{i}P(Y=c_i)\prod\limits_{j}P(X^{(j)}=x^{(j)}|Y=c_i)} ,~~~k=1,2,\cdots,K
$$

于是朴素贝叶斯分类器可表示为：

$$
y
=f(x)
=\argmax_{c_k}\frac{P(Y=c_k)\prod\limits_{j} P(X^{(j)}=x^{(j)}|Y=c_k) }{\sum\limits_{i}P(Y=c_i)\prod\limits_{j}P(X^{(j)}=x^{(j)}|Y=c_i)}
$$

注意到，上式中，对所有的 $c_k,$ 分母的值都是相同的，于是,朴素贝叶斯分类器可进一步简化为：

$$
y
=\argmax_{c_k}P(Y=c_k)\prod\limits_{j} P(X^{(j)}=x^{(j)}|Y=c_k) 
$$

### 后验概率最大化

朴素贝叶斯把实例分到后验概率最大的类中，这等价于期望风险最小化









